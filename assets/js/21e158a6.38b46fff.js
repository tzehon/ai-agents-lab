"use strict";(self.webpackChunkai_agents_lab=self.webpackChunkai_agents_lab||[]).push([[264],{6726:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(4848),s=t(8453);const r={},i="\ud83d\udcd8 Lecture notes",o={id:"create-agent/lecture-notes",title:"\ud83d\udcd8 Lecture notes",description:"Creating agents using LangGraph",source:"@site/docs/50-create-agent/1-lecture-notes.mdx",sourceDirName:"50-create-agent",slug:"/create-agent/lecture-notes",permalink:"/ai-agents-lab/docs/create-agent/lecture-notes",draft:!1,unlisted:!1,editUrl:"https://github.com/tzehon/ai-agents-lab/blob/main/docs/50-create-agent/1-lecture-notes.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Create Agent",permalink:"/ai-agents-lab/docs/category/create-agent"},next:{title:"\ud83d\udc50 Define graph state",permalink:"/ai-agents-lab/docs/create-agent/define-graph-state"}},h={},c=[{value:"Creating agents using LangGraph",id:"creating-agents-using-langgraph",level:2},{value:"Nodes",id:"nodes",level:3},{value:"Edges",id:"edges",level:3},{value:"State",id:"state",level:3},{value:"Using different LLM providers with LangChain",id:"using-different-llm-providers-with-langchain",level:2}];function l(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"-lecture-notes",children:"\ud83d\udcd8 Lecture notes"})}),"\n",(0,a.jsx)(n.h2,{id:"creating-agents-using-langgraph",children:"Creating agents using LangGraph"}),"\n",(0,a.jsx)(n.p,{children:"In this lab, we will use LangGraph by LangChain to orchestrate an AI agent for a technical documentation website. LangGraph allows you to model agentic systems as graphs. Graphs in LangGraph have the following core features:"}),"\n",(0,a.jsx)(n.h3,{id:"nodes",children:"Nodes"}),"\n",(0,a.jsx)(n.p,{children:"Nodes in LangGraph are Python functions that encode the logic of your agents. They receive the current state of the graph as input, perform some computation and return an updated state."}),"\n",(0,a.jsx)(n.h3,{id:"edges",children:"Edges"}),"\n",(0,a.jsx)(n.p,{children:"Edges in LangGraph determine which graph node to execute next based on the current state of the graph. Edges can be conditional, fixed and even result in loops."}),"\n",(0,a.jsx)(n.h3,{id:"state",children:"State"}),"\n",(0,a.jsx)(n.p,{children:"Each graph has a state which is a shared data structure that all the nodes can access and make updates to. You can define custom attributes within the state depending on what parameters you want to track across the nodes of the graph."}),"\n",(0,a.jsxs)(n.p,{children:["To learn more about these concepts, refer to the ",(0,a.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/concepts/low_level/",children:"LangGraph docs"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"using-different-llm-providers-with-langchain",children:"Using different LLM providers with LangChain"}),"\n",(0,a.jsx)(n.p,{children:"LangChain supports different LLM providers for you to build AI applications with. Unless you are using open-source models, you typically need to obtain API keys to use the chat completion APIs offered by different LLM providers."}),"\n",(0,a.jsx)(n.p,{children:"For this lab, we have created a serverless function that creates LLM objects for Amazon, Google and Microsoft models that you can use with LangChain and LangGraph without having to obtain API keys. However, if you would like to do this on your own, here are some resources:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/llms/bedrock/",children:"Using Amazon Bedrock LLMs with LangChain"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/google_generative_ai/",children:"Using Google LLMs with LangChain"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/azure_chat_openai/",children:"Using Microsoft LLMs with langChain"})}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(6540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);