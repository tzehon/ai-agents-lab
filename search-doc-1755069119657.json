{"searchDocs":[{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"About the data‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#about-the-data","content":" In this lab, we are using a serverless function to import the data required by the agent's tools, into MongoDB. If you want to do this on your own, these datasets are available on Hugging Face:  mongodb-docs: Markdown versions of a small subset of MongoDB's technical documentation. This dataset is imported into a collection called full_docs. mongodb-docs-embedded: Chunked and embedded versions of the articles in the mongodb-docs dataset. This dataset is imported into a collection called chunked_docs.  To learn more about chunking and embedding, here are some resources from our Developer Center:  How to Choose the Right Chunking Strategy for Your LLM ApplicationHow to Choose the Best Embedding Model for Your LLM Application  ","version":"Next","tagName":"h2"},{"title":"Tool calling‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#tool-calling","content":" Tool calling, interchangeably called function calling allows an LLM to use external tools such as APIs, databases, specialized machine learning models etc.  In AI agents, an LLM can have access to multiple tools. Given a user query, the LLM decides which tool to invoke and the arguments for the tool call. These arguments are used to execute the tool call and the output is returned back to the LLM to inform its next steps.  The easiest way to define tools in LangChain is using the @tool decorator. The decorator makes tools out of functions by using the function name as the tool name by default, and the function's docstring as the tool's description. The tool call inturn consists of a tool name, arguments, and an optional identifier.  An example of a tool in LangChain is as follows:  @tool(&quot;search-tool&quot;, return_direct=True) def search(query: str) -&gt; str: &quot;&quot;&quot;Look up things online.&quot;&quot;&quot; return &quot;MongoDB&quot;   An example of a tool call is as follows:  { &quot;name&quot;: &quot;search-tool&quot;, &quot;args&quot;: { &quot;query&quot;: &quot;What is MongoDB?&quot; }, &quot;id&quot;: &quot;call_H5TttXb423JfoulF1qVfPN3m&quot; }   ","version":"Next","tagName":"h2"},{"title":"Vector Search in MongoDB‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#vector-search-in-mongodb","content":" You can learn more about vector search in MongoDB here. ","version":"Next","tagName":"h2"},{"title":"üëê Add memory to the agent","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/adding-memory","content":"üëê Add memory to the agent The final step in this lab is to add conversational message history as a form of memory for the agent. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 11: Add memory to the agent section in the notebook to add memory to the agent. The answers for code blocks in this section are as follows: CODE_BLOCK_19 Answer {&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}} ","keywords":"","version":"Next"},{"title":"üëê Create a vector search index","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-vector-search-index","content":"üëê Create a vector search index To retrieve documents using vector search, you must configure a vector search index on the collection you want to perform vector search against. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 3: Create a vector search index section in the notebook to create a vector search index. The answers for code blocks in this section are as follows: CODE_BLOCK_1 Answer create_index(vs_collection, VS_INDEX_NAME, model) ","keywords":"","version":"Next"},{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Checkpoints‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#checkpoints","content":" Checkpoints in LangGraph are a snapshot of the graph state. This is how AI applications built using LangGraph persist short-term and long-term memory.  ","version":"Next","tagName":"h2"},{"title":"Thread IDs‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#thread-ids","content":" Thread IDs are unique IDs assigned to memory checkpoints in LangGraph, allowing it to distinguish between conversation threads, facilitate human-in-the loop workflows and allow users to review and debug graph executions.  ","version":"Next","tagName":"h2"},{"title":"Persisting checkpoints in MongoDB‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#persisting-checkpoints-in-mongodb","content":" In this lab, you will persist the short-term memory of the agent in MongoDB. To do this, compile the agent's graph with the MongoDB checkpointer available in our LangGraph integration as follows:  from langgraph.checkpoint.mongodb import MongoDBSaver checkpointer = MongoDBSaver(mongodb_client) app = graph.compile(checkpointer=checkpointer)   Refer to this documentation to learn more about the MongoDB checkpointer for LangGraph. ","version":"Next","tagName":"h2"},{"title":"üëê Define graph nodes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-nodes","content":"üëê Define graph nodes Let's define the nodes of the graph. The agent will have two nodes- an agent node and a tool node. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 7: Define graph nodes section in the notebook to define the nodes of the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_11 Answer state[&quot;messages&quot;] CODE_BLOCK_12 Answer llm_with_tools.invoke(messages) CODE_BLOCK_13 Answer tool.invoke(tool_call[&quot;args&quot;]) ","keywords":"","version":"Next"},{"title":"üëê Define conditional edges","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-conditional-edges","content":"üëê Define conditional edges Edges in a LangGraph graph can be fixed or conditional. For conditional edges, we need a routing function to conditionally route the workflow to different nodes. Run the cells under the Step 8: Define conditional edges section in the notebook to define the routing function for the one conditional edge in the graph.","keywords":"","version":"Next"},{"title":"üëê Define graph state","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-state","content":"üëê Define graph state Let's start by defining the state of the agent's graph. Run the cells under the Step 5: Define graph state section in the notebook to define the graph state for the AI agent.","keywords":"","version":"Next"},{"title":"üëê Create agent tools","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools","content":"","keywords":"","version":"Next"},{"title":"Vector search tool‚Äã","type":1,"pageTitle":"üëê Create agent tools","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools#vector-search-tool","content":" CODE_BLOCK_2  Answer embedding_model.encode(text)   CODE_BLOCK_3  Answer get_embedding(user_query)   CODE_BLOCK_4  Answer [ { &quot;$vectorSearch&quot;: { &quot;index&quot;: VS_INDEX_NAME, &quot;path&quot;: &quot;embedding&quot;, &quot;queryVector&quot;: query_embedding, &quot;numCandidates&quot;: 150, &quot;limit&quot;: 5, } }, { &quot;$project&quot;: { &quot;_id&quot;: 0, &quot;body&quot;: 1, &quot;score&quot;: {&quot;$meta&quot;: &quot;vectorSearchScore&quot;}, } }, ]   CODE_BLOCK_5  Answer vs_collection.aggregate(pipeline)   ","version":"Next","tagName":"h2"},{"title":"Get page content tool‚Äã","type":1,"pageTitle":"üëê Create agent tools","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools#get-page-content-tool","content":" CODE_BLOCK_6  Answer {&quot;title&quot;: user_query}   CODE_BLOCK_7  Answer {&quot;_id&quot;: 0, &quot;body&quot;: 1}   CODE_BLOCK_8  Answer full_collection.find_one(query, projection)  ","version":"Next","tagName":"h2"},{"title":"üëê Instantiate the LLM","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/instantiate-llm","content":"üëê Instantiate the LLM Now let's instantiate the LLM that will serve as the &quot;brain&quot; of the agent, and give it access to the tools we defined previously. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 6: Instantiate the LLM section in the notebook to initialize the LLM for the agent and give it access to tools. The answers for code blocks in this section are as follows: CODE_BLOCK_9 Answer llm.bind_tools(tools) CODE_BLOCK_10 Answer prompt | bind_tools ","keywords":"","version":"Next"},{"title":"üëê Setup prerequisites","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/dev-env/setup-pre-reqs","content":"üëê Setup prerequisites Select the LLM provider recommended by your instructor, and run the cells under the Step 1: Setup prerequisites section in the notebook. info Additional steps if you are running the lab locally: Spin up a MongoDB Atlas cluster and obtain its connection string: Register for a free MongoDB Atlas account if you don't already have oneCreate a new database clusterObtain the connection string for your database cluster Set the MONGODB_URI variable to the connection string for your cluster as follows: MONGODB_URI = &quot;&lt;your_connection_string&gt;&quot; Manually set the value of the SERVERLESS_URL variable as follows: SERVERLESS_URL = &quot;https://vtqjvgchmwcjwsrela2oyhlegu0hwqnw.lambda-url.us-west-2.on.aws/&quot; ","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/intro","content":"Introduction Lab goals\tLearn the basics of building AI agentsWhat you'll learn\tWhat are AI agents When to use AI agents? Components of an AI agent Agent Architectures Building an AI agent Adding memory to agents Time to complete\t90 mins In the navigation bar and in some pages, you will notice some icons. Here is their meaning: Icon\tMeaning\tDescriptionüìò\tLecture material\tIf you are following along in an instructor-led session, they probably have covered this already. üëê\tHands-on content\tGet ready to do some hands-on work. You should follow these steps. üìö\tDocumentation\tReference documentation for hands-on portions of the lab. ü¶π\tAdvanced content\tThis content isn't covered during the lab, but if you're interested in learning more, you can check it out.","keywords":"","version":"Next"},{"title":"üëê Build and execute the graph","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/build-and-execute-graph","content":"üëê Build and execute the graph Now that we have defined the nodes and edges of the graph, let's put the graph together and execute it to ensure that the agent is working as expected. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 9: Build the graph and Step 10: Execute the graph sections in the notebook to build and execute the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_14 Answer graph.add_node(&quot;agent&quot;, agent) CODE_BLOCK_15 Answer graph.add_node(&quot;tools&quot;, tool_node) CODE_BLOCK_16 Answer graph.add_edge(START, &quot;agent&quot;) CODE_BLOCK_17 Answer graph.add_edge(&quot;tools&quot;, &quot;agent&quot;) CODE_BLOCK_18 Answer graph.add_conditional_edges( &quot;agent&quot;, route_tools, {&quot;tools&quot;: &quot;tools&quot;, END: END}, ) caution Upon executing the graph, if you see the agent get stuck in an infinite tool-calling loop, play around with the prompt in Step 7 and run the cells that follow until you initialize the llm_with_tools variable. If this doesn't work, consider this a lesson in working with non-deterministic ML models. üôÇ","keywords":"","version":"Next"},{"title":"ü¶π Agent architectures","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/agent-architectures","content":"","keywords":"","version":"Next"},{"title":"Tool-calling‚Äã","type":1,"pageTitle":"ü¶π Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#tool-calling","content":" This is the architecture most agents start with. It consists of a single LLM that has access to several tools to perform a range of tasks.    If you start with tool-calling agents, but upon thorough evaluation find that you need a more sophisticated architecture, only then consider multi-agent architectures. Bear in mind that fully autonomous multi-agent workflows mean higher costs, latency, and a system that is hard to debug, so use them with caution.  ","version":"Next","tagName":"h2"},{"title":"Supervisor‚Äã","type":1,"pageTitle":"ü¶π Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#supervisor","content":" In this architecture, a single agent (supervisor) interfaces with a group of agents to determine the next course of action.    ","version":"Next","tagName":"h2"},{"title":"Network‚Äã","type":1,"pageTitle":"ü¶π Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#network","content":" In this architecture, each agent can communicate with every other agent and decide which one to call next or end the execution.    ","version":"Next","tagName":"h2"},{"title":"Custom‚Äã","type":1,"pageTitle":"ü¶π Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#custom","content":" In this setup, you can decide which agents can interact with each other and how the control flows between them.   ","version":"Next","tagName":"h2"},{"title":"üìò When to use AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents","content":"üìò When to use AI agents? AI agents are best suited for complex, multi-step tasks that require integration of multiple capabilities, such as question-answering, analysis, task execution etc. to arrive at the final answer or outcome. An active area of research is to have AI agents learn from their past interactions to build personalized and adaptive experiences. Here are some examples of tasks/questions that DO NOT require an AI agent: Who was the first president of the United States? The information required to answer this question is very likely present in the parametric knowledge of most LLMs. Hence, this question can be answer using a simple prompt to an LLM. What is the reimbursement policy for meals for my company? The information required to answer this question is most likely not present in the parametric knowledge of available LLMs. However, this question can easily be answered using Retrieval Augmented Generation (RAG) using a knowledge base consisting of your company's data. This still does not require an agent. Here are some use cases for AI agents: How has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates? Additionally, can you provide a graphical representation of the trend in obesity rates over this period? This question involves multiple sub-tasks such as data aggregation, visualization, and reasoning. Hence, this is a good use case for an AI agent. Creating a personalized learning assistant that can adjust its language, examples, and methods based on the student‚Äôs responses. This is an example of a complex task which also involves user personalization. Hence, this is a good fit for an AI agent.","keywords":"","version":"Next"},{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Creating agents using LangGraph‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#creating-agents-using-langgraph","content":" In this lab, we will use LangGraph by LangChain to orchestrate an AI agent for a technical documentation website. LangGraph allows you to model agentic systems as graphs. Graphs in LangGraph have the following core features:  ","version":"Next","tagName":"h2"},{"title":"Nodes‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#nodes","content":" Nodes in LangGraph are Python functions that encode the logic of your agents. They receive the current state of the graph as input, perform some computation and return an updated state.  ","version":"Next","tagName":"h3"},{"title":"Edges‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#edges","content":" Edges in LangGraph determine which graph node to execute next based on the current state of the graph. Edges can be conditional, fixed and even result in loops.  ","version":"Next","tagName":"h3"},{"title":"State‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#state","content":" Each graph has a state which is a shared data structure that all the nodes can access and make updates to. You can define custom attributes within the state depending on what parameters you want to track across the nodes of the graph.  To learn more about these concepts, refer to the LangGraph docs.  ","version":"Next","tagName":"h3"},{"title":"Using different LLM providers with LangChain‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#using-different-llm-providers-with-langchain","content":" LangChain supports different LLM providers for you to build AI applications with. Unless you are using open-source models, you typically need to obtain API keys to use the chat completion APIs offered by different LLM providers.  For this lab, we have created a serverless function that creates LLM objects for Amazon, Google and Microsoft models that you can use with LangChain and LangGraph without having to obtain API keys. However, if you would like to do this on your own, here are some resources:  Using Amazon Bedrock LLMs with LangChain Using Google LLMs with LangChain Using Microsoft LLMs with langChain ","version":"Next","tagName":"h2"},{"title":"üëê Import data into MongoDB","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/import-data/import-data","content":"üëê Import data into MongoDB The documentation agent has two tools- a vector search tool to retrieve information from documentation to answer questions, and another tool to get the content from specific documentation pages for summarization. Let's import the data required by these tools into two MongoDB collections. Run the cells under the Step 2: Import data into MongoDB section in the notebook to import the data required by the agent's tools, into MongoDB collections. To visually verify that the data has been imported into the local MongoDB cluster, click the leaf icon in the left navigation bar of the IDE. Ensure that you see a database called mongodb_genai_devday_agents, and two collections named mongodb_docs and mongodb_docs_embeddings under it. Click the &gt; arrow next to each collection and note the number of documents in it.","keywords":"","version":"Next"},{"title":"üéØ Summary","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/summary","content":"üéØ Summary Congratulations! Following this lab, you have successfully: learned what are AI agentslearned when to use AI agentslearned about different agent architecturesbuilt an AI agent with memory Here are some resources that you might find helpful: AI Learning HubGenAI Code Examples RepositoryGenAI Community Forums","keywords":"","version":"Next"},{"title":"üìò Components of AI agents","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/components-of-agents","content":"","keywords":"","version":"Next"},{"title":"Perception‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#perception","content":" Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs.  ","version":"Next","tagName":"h2"},{"title":"Planning and reasoning‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#planning-and-reasoning","content":" AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed.  Some common design patterns for planning and reasoning in AI agents are as follows:  ","version":"Next","tagName":"h2"},{"title":"Chain of Thought (Cot) Prompting‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#chain-of-thought-cot-prompting","content":" In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem.  Here is an example of a zero-shot CoT prompt:  Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.  ","version":"Next","tagName":"h3"},{"title":"ReAct (Reason + Act)‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#react-reason--act","content":" In this approach, the LLM is prompted to generate reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans, while actions allow it to interface with external sources or tools, to gather additional information.  Here is an example of a ReAct prompt:  Answer the following questions as best you can. You have access to the following tools:{tools} ## Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question   ","version":"Next","tagName":"h3"},{"title":"Reflection‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#reflection","content":" Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality.  ","version":"Next","tagName":"h3"},{"title":"Tools‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#tools","content":" Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models.  ","version":"Next","tagName":"h2"},{"title":"Memory‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#memory","content":" The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions.  There are two main types of memory for AI agents:  Short-term memory: Stores and retrieves information from a specific conversation. Long-term memory: Stores, retrieves and updates information based on multiple conversations had over a period of time. ","version":"Next","tagName":"h2"},{"title":"üìò What are AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/what-are-ai-agents","content":"üìò What are AI agents? An AI agent is a system that uses an LLM to reason through a problem, create a plan to solve the problem, and execute and iterate on the plan with the help of a set of tools.","keywords":"","version":"Next"},{"title":"üëê Setup dev environment","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/dev-env/dev-env-setup","content":"üëê Setup dev environment ü¶π If you are doing this lab as part of a MongoDB GenAI Developer Day, you can continue in the environment you previously created. InstruqtGitHub CodespacesLocal Navigate to the Instruqt lab using this link. Click Continue to continue in the sandbox you created previously. In the Explorer menu, navigate to genai-devday-notebooks &gt; notebooks &gt; ai-agents-lab.ipynb This is the Jupyter Notebook you will be using throughout this lab. tip Notice that this documentation website is also linked in the Agents Lab Instructions tab of your Instruqt sandbox. Feel free to access the documentation from there instead for the rest of the lab. tip In this lab, we will be using Jupyter Notebooks, which is an interactive Python environment. If you are new to Jupyter Notebooks, use this guide to familiarize yourself with the environment. InstruqtGitHub CodespacesLocal Instruqt is a lab platform that provides cloud-based sandboxes which come pre-configured with all the tools you need to run this lab. Navigate to the Instruqt lab using this link. Fill out the form that appears and click Submit and access. Click Start to launch the lab environment. You should see a screen with a purple progress bar indicating that Instruqt is preparing a sandbox with all the required libraries for this lab and a MongoDB cluster. Once this is done, you should see a Start button at the bottom right of the screen. Click this to enter the lab. Connect to the MongoDB cluster‚Äã Let's first connect to the MongoDB cluster that was created for you. This will allow you to view data we import into the cluster later in the lab, directly from the VSCode IDE. To do this, click the leaf icon in the left navigation bar of the IDE. This is MongoDB's VSCode extension. Under Connections, click the Local MongoDB Atlas connection. This should automatically establish a connection to the local MongoDB cluster running on port 27017. If the connection was successful, you should see a green leaf and a &quot;connected&quot; message appear around the Local MongoDB Atlas connection. You will also see the default databases in the cluster appear under Connections. Any additional databases we create during the lab will also appear here. Jupyter Notebook setup‚Äã You will be filling code in a Jupyter Notebook during this lab, so let's get set up with that next! Within the sandbox, click on the files icon in the left navigation bar of the IDE. In the Explorer menu, navigate to genai-devday-notebooks &gt; notebooks &gt; ai-agents-lab.ipynb to open the Jupyter Notebook for this lab. Next, select the Python interpreter by clicking Select Kernel at the top right of the IDE. In the modal that appears, click Python environments... and select the interpreter that is marked as Recommended or Global Env. That's it! You're ready for the lab! tip Notice that this documentation website is also linked in the Agents Lab Instructions tab of your Instruqt sandbox. Feel free to access the documentation from there instead for the rest of the lab.","keywords":"","version":"Next"}],"options":{"id":"default"}}