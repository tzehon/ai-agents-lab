# ðŸ“˜ Components of AI agents

AI agents have four main components: **perception**, **planning and reasoning**, **tools**, and **memory**.

## Perception

Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs.

## Planning and reasoning

AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed.

Before diving into specific reasoning patterns, it's important to understand prompting strategies:

### Prompting Strategies

#### Zero-Shot Prompting
The model is given a task without any examples, relying entirely on its pre-trained knowledge.

**Example:**
```
Classify the sentiment of this text as positive, negative, or neutral:
"The product exceeded my expectations."
```

**Pros:**
- No need to craft examples
- Fast and straightforward
- Works well for simple, common tasks
- Minimal prompt engineering required

**Cons:**
- May struggle with complex or domain-specific tasks
- Less consistent output format
- No guidance on desired style or approach
- Performance varies significantly by task

#### One-Shot Prompting
The model is provided with exactly one example before performing the task.

**Example:**
```
Classify sentiment as positive, negative, or neutral.

Example:
Text: "The service was terrible and slow."
Sentiment: negative

Now classify:
Text: "The product exceeded my expectations."
Sentiment:
```

**Pros:**
- Provides clear format guidance
- Better consistency than zero-shot
- Helps establish the desired output style
- Still relatively concise

**Cons:**
- Single example may not cover edge cases
- Can bias the model toward the example's specific pattern
- May not generalize well to diverse inputs
- Limited demonstration of task complexity

#### Few-Shot / Many-Shot / Multi-Shot Prompting
The model is given multiple examples (typically 3-10 for few-shot, more for many-shot) to learn the pattern.

**Example:**
```
Classify sentiment as positive, negative, or neutral.

Example 1:
Text: "The service was terrible and slow."
Sentiment: negative

Example 2:
Text: "Amazing product, highly recommend!"
Sentiment: positive

Example 3:
Text: "The item works as described."
Sentiment: neutral

Now classify:
Text: "The product exceeded my expectations."
Sentiment:
```

**Pros:**
- Best performance on complex tasks
- Can demonstrate edge cases and nuances
- Establishes clear patterns and consistency
- Reduces ambiguity in task interpretation

**Cons:**
- Requires careful example selection
- Uses more tokens (higher cost)
- Risk of overfitting to examples
- Can hit context length limits with many examples
- Time-consuming to create good examples

### Common Design Patterns

With these prompting strategies in mind, here are common design patterns for planning and reasoning in AI agents:

### Chain of Thought (CoT) Prompting

In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem. CoT prompting helps the model break down complex problems into manageable steps and show its reasoning process transparently.

**Pros:**
- Improves accuracy on complex reasoning tasks
- Makes the model's thinking process transparent and auditable
- Helps catch logical errors in the reasoning chain
- Works well for mathematical, logical, and multi-step problems

**Cons:**
- Increases token usage and response time
- May overthink simple problems
- Can sometimes generate incorrect reasoning that leads to wrong answers
- Not necessary for straightforward tasks

**Example:**
```
Prompt: Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.

Question: A store has 120 apples. They sell 40% on Monday and 25% of the remainder on Tuesday. How many apples are left?

Response:
Step 1: Calculate apples sold on Monday
- 40% of 120 = 0.4 Ã— 120 = 48 apples sold

Step 2: Calculate remainder after Monday
- 120 - 48 = 72 apples remaining

Step 3: Calculate apples sold on Tuesday
- 25% of 72 = 0.25 Ã— 72 = 18 apples sold

Step 4: Calculate final remainder
- 72 - 18 = 54 apples left

Answer: 54 apples remain after Tuesday's sales.
```

### ReAct (Reason + Act)

While Chain of Thought focuses purely on internal reasoning, ReAct combines reasoning with action-taking. The key difference is that ReAct:

1. **Interleaves reasoning with actions**: The agent thinks, acts, observes the results, then thinks again based on new information
2. **Grounds reasoning in real-world feedback**: Unlike pure CoT which reasons in isolation, ReAct agents can validate their reasoning by interacting with tools and observing outcomes
3. **Enables dynamic adaptation**: The agent can adjust its approach based on actual results rather than theoretical reasoning alone

In ReAct, the LLM alternates between:
- **Thought**: Internal reasoning about what to do next
- **Action**: Executing a specific tool or API call
- **Observation**: Processing the results of that action

This cycle continues until the task is complete, making ReAct particularly effective for tasks that require gathering information from external sources or verifying hypotheses through interaction.

**Pros:**
- Can access real-time, up-to-date information through tools
- Self-corrects based on actual feedback
- More reliable for tasks requiring external data
- Reduces hallucination by grounding responses in tool outputs

**Cons:**
- More complex to implement than pure CoT
- Requires careful tool design and error handling
- Can get stuck in loops if not properly constrained
- Higher latency due to multiple tool calls
- More expensive due to additional API calls and tokens

**Example:**
```
Prompt: Answer the following questions as best you can. You have access to the following tools:
- Weather: Gets current weather for a location
- Calculator: Performs mathematical calculations

Use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Weather, Calculator]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Question: What is the current temperature in Tokyo, and how much warmer is it than the freezing point of water in Celsius?

Response:
Thought: I need to first get the current temperature in Tokyo, then calculate the difference from 0Â°C (freezing point).
Action: Weather
Action Input: Tokyo, Japan
Observation: Current temperature in Tokyo is 22Â°C, partly cloudy

Thought: Now I need to calculate how much warmer 22Â°C is compared to the freezing point of 0Â°C.
Action: Calculator
Action Input: 22 - 0
Observation: 22

Thought: I now know the final answer
Final Answer: The current temperature in Tokyo is 22Â°C, which is 22Â°C warmer than the freezing point of water (0Â°C).
```

Notice how the agent cycles through Thought â†’ Action â†’ Observation multiple times, using each observation to inform the next thought and action. This demonstrates the key difference from Chain of Thought, which would reason through the entire problem internally without external tool interactions.

### Reflection

Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality.

**Pros:**
- Improves output quality through iterative refinement
- Can catch and correct mistakes before final output
- Particularly effective for creative and analytical tasks
- Reduces errors through self-critique

**Cons:**
- Significantly increases computational cost and latency
- May over-revise and lose good initial insights
- Not always necessary for simple tasks
- Can introduce new errors during revision

**Example:**
```
Prompt: You will iteratively improve your response through reflection. Follow this format:
1. Initial Attempt: Your first response
2. Reflection: Critique your attempt, identifying strengths and weaknesses
3. Revised Attempt: An improved version based on your reflection
4. Final Reflection: Evaluate if further improvements are needed
5. Final Answer: Your best version

Question: Write a haiku about artificial intelligence

Response:
Initial Attempt:
Circuits pulse with light
Digital minds awakening
Future now unfolds

Reflection: Let me review this haiku. It's 5-8-5 instead of 5-7-5. I need to fix "awakening".

Revised Attempt:
Silicon dreams deep
Patterns learned from human words
Teaching us ourselves

Final Reflection: This version is better - it's less clichÃ© and includes the meta aspect of AI learning from humans. The last line has a nice philosophical touch about AI reflecting humanity back to us. The syllable count is correct (5-7-5). This captures both the technical and philosophical aspects of AI effectively.

Final Answer:
Silicon dreams deep
Patterns learned from human words
Teaching us ourselves
```

Notice how the agent generates, critiques, and refines its output through multiple iterations, improving quality with each reflection cycle.

### Combining Patterns

These patterns can be combined for more sophisticated reasoning. Here's an example using all three:

**Example: CoT + ReAct + Reflection Combined**
```
Prompt: You will solve problems using a combination of reasoning patterns:
1. Use Chain of Thought for complex calculations
2. Use ReAct format when you need external tools
3. Use Reflection to verify and improve your final answer

Available tools: [Weather, Calculator, UnitConverter]

Question: If it's currently 28Â°C in Singapore and -5Â°C in Toronto, what's the temperature difference in Fahrenheit? Also, at which temperature would water boil at sea level in both cities?

Response:
Initial Approach:
Thought: I need to get the temperatures, convert them, and calculate the boiling point. Let me start by understanding what I need to do.

Step 1 (CoT): Break down the problem
- Get current temperatures (already provided: Singapore 28Â°C, Toronto -5Â°C)
- Calculate temperature difference in Celsius
- Convert difference to Fahrenheit
- Determine boiling point of water at sea level

Step 2 (ReAct): Execute with tools
Thought: First, let me calculate the temperature difference in Celsius
Action: Calculator
Action Input: 28 - (-5)
Observation: 33

Thought: Now I need to convert this 33Â°C difference to Fahrenheit. The formula is (C Ã— 9/5) + 32
Action: Calculator
Action Input: (33 * 9 / 5) + 32
Observation: 91.4

Thought: Now I need to get the boiling point of water at sea level
Action: UnitConverter
Action Input: 100Â°C to Fahrenheit
Observation: 212Â°F

Step 3 (Reflection): Review my work
Initial Answer: The temperature difference is 91.4Â°F, and water boils at 212Â°F (100Â°C) at sea level in both cities.

Reflection: Wait, I should verify my calculation. For temperature differences, the conversion is 59.4Â°F. The boiling point is correct at sea level. However, I should clarify that the boiling point is the same in both cities since they're both at approximately sea level.

Final Answer:
- Temperature difference: 91.4Â°F (33Â°C difference)
- Water boils at 100Â°C (212Â°F) at sea level in both Singapore and Toronto
```

This example demonstrates how:
- **Chain of Thought** helps break down the complex problem into steps
- **ReAct** enables interaction with tools for calculations and conversions
- **Reflection** catches potential ambiguities and improves the final answer

These patterns work synergistically: CoT provides the planning, ReAct executes the plan with real-world interactions, and Reflection ensures quality and accuracy.

## Tools

Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models.

## Memory

The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions.

There are two main types of memory for AI agents:

* **Short-term memory**: Stores and retrieves information from a specific conversation.

* **Long-term memory**: Stores, retrieves and updates information based on multiple conversations had over a period of time.