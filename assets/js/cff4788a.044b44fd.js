"use strict";(self.webpackChunkai_agents_lab=self.webpackChunkai_agents_lab||[]).push([[418],{6768:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var s=t(4848),i=t(8453);const o={},r="\ud83d\udcd8 Components of AI agents",l={id:"key-concepts/components-of-agents",title:"\ud83d\udcd8 Components of AI agents",description:"AI agents have four main components: perception, planning and reasoning, tools, and memory.",source:"@site/docs/10-key-concepts/5-components-of-agents.mdx",sourceDirName:"10-key-concepts",slug:"/key-concepts/components-of-agents",permalink:"/ai-agents-lab/docs/key-concepts/components-of-agents",draft:!1,unlisted:!1,editUrl:"https://github.com/tzehon/ai-agents-lab/blob/main/docs/10-key-concepts/5-components-of-agents.mdx",tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udcd8 When to use AI agents?",permalink:"/ai-agents-lab/docs/key-concepts/when-to-use-agents"},next:{title:"\ud83e\uddb9 Agent architectures",permalink:"/ai-agents-lab/docs/key-concepts/agent-architectures"}},a={},c=[{value:"Perception",id:"perception",level:2},{value:"Planning and reasoning",id:"planning-and-reasoning",level:2},{value:"Prompting Strategies",id:"prompting-strategies",level:3},{value:"Zero-Shot Prompting",id:"zero-shot-prompting",level:4},{value:"One-Shot Prompting",id:"one-shot-prompting",level:4},{value:"Few-Shot / Many-Shot / Multi-Shot Prompting",id:"few-shot--many-shot--multi-shot-prompting",level:4},{value:"Common Design Patterns",id:"common-design-patterns",level:3},{value:"Chain of Thought (CoT) Prompting",id:"chain-of-thought-cot-prompting",level:3},{value:"ReAct (Reason + Act)",id:"react-reason--act",level:3},{value:"Reflection",id:"reflection",level:3},{value:"Tools",id:"tools",level:2},{value:"Memory",id:"memory",level:2}];function h(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"-components-of-ai-agents",children:"\ud83d\udcd8 Components of AI agents"})}),"\n",(0,s.jsxs)(n.p,{children:["AI agents have four main components: ",(0,s.jsx)(n.strong,{children:"perception"}),", ",(0,s.jsx)(n.strong,{children:"planning and reasoning"}),", ",(0,s.jsx)(n.strong,{children:"tools"}),", and ",(0,s.jsx)(n.strong,{children:"memory"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"perception",children:"Perception"}),"\n",(0,s.jsx)(n.p,{children:"Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs."}),"\n",(0,s.jsx)(n.h2,{id:"planning-and-reasoning",children:"Planning and reasoning"}),"\n",(0,s.jsx)(n.p,{children:"AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed."}),"\n",(0,s.jsx)(n.p,{children:"Before diving into specific reasoning patterns, it's important to understand prompting strategies:"}),"\n",(0,s.jsx)(n.h3,{id:"prompting-strategies",children:"Prompting Strategies"}),"\n",(0,s.jsx)(n.h4,{id:"zero-shot-prompting",children:"Zero-Shot Prompting"}),"\n",(0,s.jsx)(n.p,{children:"The model is given a task without any examples, relying entirely on its pre-trained knowledge."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Classify the sentiment of this text as positive, negative, or neutral:\n"The product exceeded my expectations."\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No need to craft examples"}),"\n",(0,s.jsx)(n.li,{children:"Fast and straightforward"}),"\n",(0,s.jsx)(n.li,{children:"Works well for simple, common tasks"}),"\n",(0,s.jsx)(n.li,{children:"Minimal prompt engineering required"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"May struggle with complex or domain-specific tasks"}),"\n",(0,s.jsx)(n.li,{children:"Less consistent output format"}),"\n",(0,s.jsx)(n.li,{children:"No guidance on desired style or approach"}),"\n",(0,s.jsx)(n.li,{children:"Performance varies significantly by task"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"one-shot-prompting",children:"One-Shot Prompting"}),"\n",(0,s.jsx)(n.p,{children:"The model is provided with exactly one example before performing the task."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Classify sentiment as positive, negative, or neutral.\n\nExample:\nText: "The service was terrible and slow."\nSentiment: negative\n\nNow classify:\nText: "The product exceeded my expectations."\nSentiment:\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provides clear format guidance"}),"\n",(0,s.jsx)(n.li,{children:"Better consistency than zero-shot"}),"\n",(0,s.jsx)(n.li,{children:"Helps establish the desired output style"}),"\n",(0,s.jsx)(n.li,{children:"Still relatively concise"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Single example may not cover edge cases"}),"\n",(0,s.jsx)(n.li,{children:"Can bias the model toward the example's specific pattern"}),"\n",(0,s.jsx)(n.li,{children:"May not generalize well to diverse inputs"}),"\n",(0,s.jsx)(n.li,{children:"Limited demonstration of task complexity"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"few-shot--many-shot--multi-shot-prompting",children:"Few-Shot / Many-Shot / Multi-Shot Prompting"}),"\n",(0,s.jsx)(n.p,{children:"The model is given multiple examples (typically 3-10 for few-shot, more for many-shot) to learn the pattern."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Classify sentiment as positive, negative, or neutral.\n\nExample 1:\nText: "The service was terrible and slow."\nSentiment: negative\n\nExample 2:\nText: "Amazing product, highly recommend!"\nSentiment: positive\n\nExample 3:\nText: "The item works as described."\nSentiment: neutral\n\nNow classify:\nText: "The product exceeded my expectations."\nSentiment:\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Best performance on complex tasks"}),"\n",(0,s.jsx)(n.li,{children:"Can demonstrate edge cases and nuances"}),"\n",(0,s.jsx)(n.li,{children:"Establishes clear patterns and consistency"}),"\n",(0,s.jsx)(n.li,{children:"Reduces ambiguity in task interpretation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requires careful example selection"}),"\n",(0,s.jsx)(n.li,{children:"Uses more tokens (higher cost)"}),"\n",(0,s.jsx)(n.li,{children:"Risk of overfitting to examples"}),"\n",(0,s.jsx)(n.li,{children:"Can hit context length limits with many examples"}),"\n",(0,s.jsx)(n.li,{children:"Time-consuming to create good examples"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"common-design-patterns",children:"Common Design Patterns"}),"\n",(0,s.jsx)(n.p,{children:"With these prompting strategies in mind, here are common design patterns for planning and reasoning in AI agents:"}),"\n",(0,s.jsx)(n.h3,{id:"chain-of-thought-cot-prompting",children:"Chain of Thought (CoT) Prompting"}),"\n",(0,s.jsx)(n.p,{children:"In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem. CoT prompting helps the model break down complex problems into manageable steps and show its reasoning process transparently."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Improves accuracy on complex reasoning tasks"}),"\n",(0,s.jsx)(n.li,{children:"Makes the model's thinking process transparent and auditable"}),"\n",(0,s.jsx)(n.li,{children:"Helps catch logical errors in the reasoning chain"}),"\n",(0,s.jsx)(n.li,{children:"Works well for mathematical, logical, and multi-step problems"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Increases token usage and response time"}),"\n",(0,s.jsx)(n.li,{children:"May overthink simple problems"}),"\n",(0,s.jsx)(n.li,{children:"Can sometimes generate incorrect reasoning that leads to wrong answers"}),"\n",(0,s.jsx)(n.li,{children:"Not necessary for straightforward tasks"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here is an example of a CoT prompt:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"react-reason--act",children:"ReAct (Reason + Act)"}),"\n",(0,s.jsx)(n.p,{children:"While Chain of Thought focuses purely on internal reasoning, ReAct combines reasoning with action-taking. The key difference is that ReAct:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interleaves reasoning with actions"}),": The agent thinks, acts, observes the results, then thinks again based on new information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grounds reasoning in real-world feedback"}),": Unlike pure CoT which reasons in isolation, ReAct agents can validate their reasoning by interacting with tools and observing outcomes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enables dynamic adaptation"}),": The agent can adjust its approach based on actual results rather than theoretical reasoning alone"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In ReAct, the LLM alternates between:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thought"}),": Internal reasoning about what to do next"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Executing a specific tool or API call"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observation"}),": Processing the results of that action"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This cycle continues until the task is complete, making ReAct particularly effective for tasks that require gathering information from external sources or verifying hypotheses through interaction."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Can access real-time, up-to-date information through tools"}),"\n",(0,s.jsx)(n.li,{children:"Self-corrects based on actual feedback"}),"\n",(0,s.jsx)(n.li,{children:"More reliable for tasks requiring external data"}),"\n",(0,s.jsx)(n.li,{children:"Reduces hallucination by grounding responses in tool outputs"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"More complex to implement than pure CoT"}),"\n",(0,s.jsx)(n.li,{children:"Requires careful tool design and error handling"}),"\n",(0,s.jsx)(n.li,{children:"Can get stuck in loops if not properly constrained"}),"\n",(0,s.jsx)(n.li,{children:"Higher latency due to multiple tool calls"}),"\n",(0,s.jsx)(n.li,{children:"More expensive due to additional API calls and tokens"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here is an example of a ReAct prompt:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Answer the following questions as best you can. You have access to the following tools:{tools}\n##\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n"})}),"\n",(0,s.jsx)(n.h3,{id:"reflection",children:"Reflection"}),"\n",(0,s.jsx)(n.p,{children:"Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Improves output quality through iterative refinement"}),"\n",(0,s.jsx)(n.li,{children:"Can catch and correct mistakes before final output"}),"\n",(0,s.jsx)(n.li,{children:"Particularly effective for creative and analytical tasks"}),"\n",(0,s.jsx)(n.li,{children:"Reduces errors through self-critique"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Significantly increases computational cost and latency"}),"\n",(0,s.jsx)(n.li,{children:"May over-revise and lose good initial insights"}),"\n",(0,s.jsx)(n.li,{children:"Not always necessary for simple tasks"}),"\n",(0,s.jsx)(n.li,{children:"Can introduce new errors during revision"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,s.jsx)(n.p,{children:"Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models."}),"\n",(0,s.jsx)(n.h2,{id:"memory",children:"Memory"}),"\n",(0,s.jsx)(n.p,{children:"The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions."}),"\n",(0,s.jsx)(n.p,{children:"There are two main types of memory for AI agents:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Short-term memory"}),": Stores and retrieves information from a specific conversation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Long-term memory"}),": Stores, retrieves and updates information based on multiple conversations had over a period of time."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);