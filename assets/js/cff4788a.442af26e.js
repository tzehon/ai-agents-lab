"use strict";(self.webpackChunkai_agents_lab=self.webpackChunkai_agents_lab||[]).push([[418],{6768:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var i=t(4848),o=t(8453);const s={},r="\ud83d\udcd8 Components of AI agents",a={id:"key-concepts/components-of-agents",title:"\ud83d\udcd8 Components of AI agents",description:"AI agents have four main components: perception, planning and reasoning, tools, and memory.",source:"@site/docs/10-key-concepts/5-components-of-agents.mdx",sourceDirName:"10-key-concepts",slug:"/key-concepts/components-of-agents",permalink:"/ai-agents-lab/docs/key-concepts/components-of-agents",draft:!1,unlisted:!1,editUrl:"https://github.com/tzehon/ai-agents-lab/blob/main/docs/10-key-concepts/5-components-of-agents.mdx",tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udcd8 When to use AI agents?",permalink:"/ai-agents-lab/docs/key-concepts/when-to-use-agents"},next:{title:"\ud83e\uddb9 Agent architectures",permalink:"/ai-agents-lab/docs/key-concepts/agent-architectures"}},l={},c=[{value:"Perception",id:"perception",level:2},{value:"Planning and reasoning",id:"planning-and-reasoning",level:2},{value:"Prompting Strategies",id:"prompting-strategies",level:3},{value:"Zero-Shot Prompting",id:"zero-shot-prompting",level:4},{value:"One-Shot Prompting",id:"one-shot-prompting",level:4},{value:"Few-Shot / Many-Shot / Multi-Shot Prompting",id:"few-shot--many-shot--multi-shot-prompting",level:4},{value:"Common Design Patterns",id:"common-design-patterns",level:3},{value:"Chain of Thought (CoT) Prompting",id:"chain-of-thought-cot-prompting",level:3},{value:"ReAct (Reason + Act)",id:"react-reason--act",level:3},{value:"Reflection",id:"reflection",level:3},{value:"Combining Patterns",id:"combining-patterns",level:3},{value:"Tools",id:"tools",level:2},{value:"Memory",id:"memory",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-components-of-ai-agents",children:"\ud83d\udcd8 Components of AI agents"})}),"\n",(0,i.jsxs)(n.p,{children:["AI agents have four main components: ",(0,i.jsx)(n.strong,{children:"perception"}),", ",(0,i.jsx)(n.strong,{children:"planning and reasoning"}),", ",(0,i.jsx)(n.strong,{children:"tools"}),", and ",(0,i.jsx)(n.strong,{children:"memory"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"perception",children:"Perception"}),"\n",(0,i.jsx)(n.p,{children:"Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs."}),"\n",(0,i.jsx)(n.h2,{id:"planning-and-reasoning",children:"Planning and reasoning"}),"\n",(0,i.jsx)(n.p,{children:"AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed."}),"\n",(0,i.jsx)(n.p,{children:"Before diving into specific reasoning patterns, it's important to understand prompting strategies:"}),"\n",(0,i.jsx)(n.h3,{id:"prompting-strategies",children:"Prompting Strategies"}),"\n",(0,i.jsx)(n.h4,{id:"zero-shot-prompting",children:"Zero-Shot Prompting"}),"\n",(0,i.jsx)(n.p,{children:"The model is given a task without any examples, relying entirely on its pre-trained knowledge."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Classify the sentiment of this text as positive, negative, or neutral:\n"The product exceeded my expectations."\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No need to craft examples"}),"\n",(0,i.jsx)(n.li,{children:"Fast and straightforward"}),"\n",(0,i.jsx)(n.li,{children:"Works well for simple, common tasks"}),"\n",(0,i.jsx)(n.li,{children:"Minimal prompt engineering required"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"May struggle with complex or domain-specific tasks"}),"\n",(0,i.jsx)(n.li,{children:"Less consistent output format"}),"\n",(0,i.jsx)(n.li,{children:"No guidance on desired style or approach"}),"\n",(0,i.jsx)(n.li,{children:"Performance varies significantly by task"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"one-shot-prompting",children:"One-Shot Prompting"}),"\n",(0,i.jsx)(n.p,{children:"The model is provided with exactly one example before performing the task."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Classify sentiment as positive, negative, or neutral.\n\nExample:\nText: "The service was terrible and slow."\nSentiment: negative\n\nNow classify:\nText: "The product exceeded my expectations."\nSentiment:\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Provides clear format guidance"}),"\n",(0,i.jsx)(n.li,{children:"Better consistency than zero-shot"}),"\n",(0,i.jsx)(n.li,{children:"Helps establish the desired output style"}),"\n",(0,i.jsx)(n.li,{children:"Still relatively concise"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Single example may not cover edge cases"}),"\n",(0,i.jsx)(n.li,{children:"Can bias the model toward the example's specific pattern"}),"\n",(0,i.jsx)(n.li,{children:"May not generalize well to diverse inputs"}),"\n",(0,i.jsx)(n.li,{children:"Limited demonstration of task complexity"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"few-shot--many-shot--multi-shot-prompting",children:"Few-Shot / Many-Shot / Multi-Shot Prompting"}),"\n",(0,i.jsx)(n.p,{children:"The model is given multiple examples (typically 3-10 for few-shot, more for many-shot) to learn the pattern."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Classify sentiment as positive, negative, or neutral.\n\nExample 1:\nText: "The service was terrible and slow."\nSentiment: negative\n\nExample 2:\nText: "Amazing product, highly recommend!"\nSentiment: positive\n\nExample 3:\nText: "The item works as described."\nSentiment: neutral\n\nNow classify:\nText: "The product exceeded my expectations."\nSentiment:\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Best performance on complex tasks"}),"\n",(0,i.jsx)(n.li,{children:"Can demonstrate edge cases and nuances"}),"\n",(0,i.jsx)(n.li,{children:"Establishes clear patterns and consistency"}),"\n",(0,i.jsx)(n.li,{children:"Reduces ambiguity in task interpretation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Requires careful example selection"}),"\n",(0,i.jsx)(n.li,{children:"Uses more tokens (higher cost)"}),"\n",(0,i.jsx)(n.li,{children:"Risk of overfitting to examples"}),"\n",(0,i.jsx)(n.li,{children:"Can hit context length limits with many examples"}),"\n",(0,i.jsx)(n.li,{children:"Time-consuming to create good examples"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"common-design-patterns",children:"Common Design Patterns"}),"\n",(0,i.jsx)(n.p,{children:"With these prompting strategies in mind, here are common design patterns for planning and reasoning in AI agents:"}),"\n",(0,i.jsx)(n.h3,{id:"chain-of-thought-cot-prompting",children:"Chain of Thought (CoT) Prompting"}),"\n",(0,i.jsx)(n.p,{children:"In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem. CoT prompting helps the model break down complex problems into manageable steps and show its reasoning process transparently."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improves accuracy on complex reasoning tasks"}),"\n",(0,i.jsx)(n.li,{children:"Makes the model's thinking process transparent and auditable"}),"\n",(0,i.jsx)(n.li,{children:"Helps catch logical errors in the reasoning chain"}),"\n",(0,i.jsx)(n.li,{children:"Works well for mathematical, logical, and multi-step problems"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Increases token usage and response time"}),"\n",(0,i.jsx)(n.li,{children:"May overthink simple problems"}),"\n",(0,i.jsx)(n.li,{children:"Can sometimes generate incorrect reasoning that leads to wrong answers"}),"\n",(0,i.jsx)(n.li,{children:"Not necessary for straightforward tasks"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Prompt: Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n\nQuestion: A store has 120 apples. They sell 40% on Monday and 25% of the remainder on Tuesday. How many apples are left?\n\nResponse:\nStep 1: Calculate apples sold on Monday\n- 40% of 120 = 0.4 \xd7 120 = 48 apples sold\n\nStep 2: Calculate remainder after Monday\n- 120 - 48 = 72 apples remaining\n\nStep 3: Calculate apples sold on Tuesday  \n- 25% of 72 = 0.25 \xd7 72 = 18 apples sold\n\nStep 4: Calculate final remainder\n- 72 - 18 = 54 apples left\n\nAnswer: 54 apples remain after Tuesday's sales.\n"})}),"\n",(0,i.jsx)(n.h3,{id:"react-reason--act",children:"ReAct (Reason + Act)"}),"\n",(0,i.jsx)(n.p,{children:"While Chain of Thought focuses purely on internal reasoning, ReAct combines reasoning with action-taking. The key difference is that ReAct:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interleaves reasoning with actions"}),": The agent thinks, acts, observes the results, then thinks again based on new information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grounds reasoning in real-world feedback"}),": Unlike pure CoT which reasons in isolation, ReAct agents can validate their reasoning by interacting with tools and observing outcomes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enables dynamic adaptation"}),": The agent can adjust its approach based on actual results rather than theoretical reasoning alone"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In ReAct, the LLM alternates between:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Thought"}),": Internal reasoning about what to do next"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action"}),": Executing a specific tool or API call"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observation"}),": Processing the results of that action"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This cycle continues until the task is complete, making ReAct particularly effective for tasks that require gathering information from external sources or verifying hypotheses through interaction."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Can access real-time, up-to-date information through tools"}),"\n",(0,i.jsx)(n.li,{children:"Self-corrects based on actual feedback"}),"\n",(0,i.jsx)(n.li,{children:"More reliable for tasks requiring external data"}),"\n",(0,i.jsx)(n.li,{children:"Reduces hallucination by grounding responses in tool outputs"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"More complex to implement than pure CoT"}),"\n",(0,i.jsx)(n.li,{children:"Requires careful tool design and error handling"}),"\n",(0,i.jsx)(n.li,{children:"Can get stuck in loops if not properly constrained"}),"\n",(0,i.jsx)(n.li,{children:"Higher latency due to multiple tool calls"}),"\n",(0,i.jsx)(n.li,{children:"More expensive due to additional API calls and tokens"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Prompt: Answer the following questions as best you can. You have access to the following tools:\n- Weather: Gets current weather for a location\n- Calculator: Performs mathematical calculations\n\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Weather, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nQuestion: What is the current temperature in Tokyo, and how much warmer is it than the freezing point of water in Celsius?\n\nResponse:\nThought: I need to first get the current temperature in Tokyo, then calculate the difference from 0\xb0C (freezing point).\nAction: Weather\nAction Input: Tokyo, Japan\nObservation: Current temperature in Tokyo is 22\xb0C, partly cloudy\n\nThought: Now I need to calculate how much warmer 22\xb0C is compared to the freezing point of 0\xb0C.\nAction: Calculator\nAction Input: 22 - 0\nObservation: 22\n\nThought: I now know the final answer\nFinal Answer: The current temperature in Tokyo is 22\xb0C, which is 22\xb0C warmer than the freezing point of water (0\xb0C).\n"})}),"\n",(0,i.jsx)(n.p,{children:"Notice how the agent cycles through Thought \u2192 Action \u2192 Observation multiple times, using each observation to inform the next thought and action. This demonstrates the key difference from Chain of Thought, which would reason through the entire problem internally without external tool interactions."}),"\n",(0,i.jsx)(n.h3,{id:"reflection",children:"Reflection"}),"\n",(0,i.jsx)(n.p,{children:"Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improves output quality through iterative refinement"}),"\n",(0,i.jsx)(n.li,{children:"Can catch and correct mistakes before final output"}),"\n",(0,i.jsx)(n.li,{children:"Particularly effective for creative and analytical tasks"}),"\n",(0,i.jsx)(n.li,{children:"Reduces errors through self-critique"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Significantly increases computational cost and latency"}),"\n",(0,i.jsx)(n.li,{children:"May over-revise and lose good initial insights"}),"\n",(0,i.jsx)(n.li,{children:"Not always necessary for simple tasks"}),"\n",(0,i.jsx)(n.li,{children:"Can introduce new errors during revision"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Prompt: You will iteratively improve your response through reflection. Follow this format:\n1. Initial Attempt: Your first response\n2. Reflection: Critique your attempt, identifying strengths and weaknesses\n3. Revised Attempt: An improved version based on your reflection\n4. Final Reflection: Evaluate if further improvements are needed\n5. Final Answer: Your best version\n\nQuestion: Write a haiku about artificial intelligence\n\nResponse:\nInitial Attempt:\nCircuits pulse with light\nDigital minds awakening  \nFuture now unfolds\n\nReflection: Let me review this haiku. It follows the 5-7-5 syllable pattern correctly. However, "awakening" might be clich\xe9 when describing AI. Also, the imagery could be more concrete and less abstract.\n\nRevised Attempt:\nSilicon dreams deep\nPatterns learned from human words\nTeaching us ourselves\n\nFinal Reflection: This version is better - it\'s less clich\xe9 and includes the meta aspect of AI learning from humans. The last line has a nice philosophical touch about AI reflecting humanity back to us. The syllable count is correct (5-7-5). This captures both the technical and philosophical aspects of AI effectively.\n\nFinal Answer: \nSilicon dreams deep\nPatterns learned from human words\nTeaching us ourselves\n'})}),"\n",(0,i.jsx)(n.p,{children:"Notice how the agent generates, critiques, and refines its output through multiple iterations, improving quality with each reflection cycle."}),"\n",(0,i.jsx)(n.h3,{id:"combining-patterns",children:"Combining Patterns"}),"\n",(0,i.jsx)(n.p,{children:"These patterns can be combined for more sophisticated reasoning. Here's an example using all three:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example: CoT + ReAct + Reflection Combined"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Prompt: You will solve problems using a combination of reasoning patterns:\n1. Use Chain of Thought for complex calculations\n2. Use ReAct format when you need external tools\n3. Use Reflection to verify and improve your final answer\n\nAvailable tools: [Weather, Calculator, UnitConverter]\n\nQuestion: If it's currently 28\xb0C in Singapore and -5\xb0C in Toronto, what's the temperature difference in Fahrenheit? Also, at which temperature would water boil at sea level in both cities?\n\nResponse:\nInitial Approach:\nThought: I need to get the temperatures, convert them, and calculate the boiling point. Let me start by understanding what I need to do.\n\nStep 1 (CoT): Break down the problem\n- Get current temperatures (already provided: Singapore 28\xb0C, Toronto -5\xb0C)\n- Calculate temperature difference in Celsius\n- Convert difference to Fahrenheit\n- Determine boiling point of water at sea level\n\nStep 2 (ReAct): Execute with tools\nThought: First, let me calculate the temperature difference in Celsius\nAction: Calculator\nAction Input: 28 - (-5)\nObservation: 33\n\nThought: Now I need to convert this 33\xb0C difference to Fahrenheit. The formula is (C \xd7 9/5) + 32, but for a difference, I only need C \xd7 9/5\nAction: Calculator  \nAction Input: 33 * 9 / 5\nObservation: 59.4\n\nThought: Now I need to get the boiling point of water at sea level\nAction: UnitConverter\nAction Input: 100\xb0C to Fahrenheit\nObservation: 212\xb0F\n\nStep 3 (Reflection): Review my work\nInitial Answer: The temperature difference is 59.4\xb0F, and water boils at 212\xb0F (100\xb0C) at sea level in both cities.\n\nReflection: Wait, I should verify my calculation. For temperature differences, the conversion is just multiplying by 9/5, which I did correctly (33 \xd7 1.8 = 59.4\xb0F). The boiling point is correct at sea level. However, I should clarify that the boiling point is the same in both cities since they're both at approximately sea level.\n\nFinal Answer: \n- Temperature difference: 59.4\xb0F (33\xb0C difference)\n- Water boils at 100\xb0C (212\xb0F) at sea level in both Singapore and Toronto\n"})}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates how:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chain of Thought"})," helps break down the complex problem into steps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ReAct"})," enables interaction with tools for calculations and conversions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reflection"})," catches potential ambiguities and improves the final answer"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These patterns work synergistically: CoT provides the planning, ReAct executes the plan with real-world interactions, and Reflection ensures quality and accuracy."}),"\n",(0,i.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,i.jsx)(n.p,{children:"Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models."}),"\n",(0,i.jsx)(n.h2,{id:"memory",children:"Memory"}),"\n",(0,i.jsx)(n.p,{children:"The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions."}),"\n",(0,i.jsx)(n.p,{children:"There are two main types of memory for AI agents:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Short-term memory"}),": Stores and retrieves information from a specific conversation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Long-term memory"}),": Stores, retrieves and updates information based on multiple conversations had over a period of time."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);