{"searchDocs":[{"title":"📘 Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Checkpoints​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#checkpoints","content":" Checkpoints in LangGraph are a snapshot of the graph state. This is how AI applications built using LangGraph persist short-term and long-term memory.  ","version":"Next","tagName":"h2"},{"title":"Thread IDs​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#thread-ids","content":" Thread IDs are unique IDs assigned to memory checkpoints in LangGraph, allowing it to distinguish between conversation threads, facilitate human-in-the loop workflows and allow users to review and debug graph executions.  ","version":"Next","tagName":"h2"},{"title":"Persisting checkpoints in MongoDB​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#persisting-checkpoints-in-mongodb","content":" In this lab, you will persist the short-term memory of the agent in MongoDB. To do this, compile the agent's graph with the MongoDB checkpointer available in our LangGraph integration as follows:  from langgraph.checkpoint.mongodb import MongoDBSaver checkpointer = MongoDBSaver(mongodb_client) app = graph.compile(checkpointer=checkpointer)   Refer to this documentation to learn more about the MongoDB checkpointer for LangGraph. ","version":"Next","tagName":"h2"},{"title":"👐 Add memory to the agent","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/adding-memory","content":"👐 Add memory to the agent The final step in this lab is to add conversational message history as a form of memory for the agent. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 11: Add memory to the agent section in the notebook to add memory to the agent. The answers for code blocks in this section are as follows: CODE_BLOCK_19 Answer {&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}} ","keywords":"","version":"Next"},{"title":"👐 Define conditional edges","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-conditional-edges","content":"👐 Define conditional edges Edges in a LangGraph graph can be fixed or conditional. For conditional edges, we need a routing function to conditionally route the workflow to different nodes. Run the cells under the Step 8: Define conditional edges section in the notebook to define the routing function for the one conditional edge in the graph.","keywords":"","version":"Next"},{"title":"👐 Define graph nodes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-nodes","content":"👐 Define graph nodes Let's define the nodes of the graph. The agent will have two nodes- an agent node and a tool node. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 7: Define graph nodes section in the notebook to define the nodes of the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_11 Answer state[&quot;messages&quot;] CODE_BLOCK_12 Answer llm_with_tools.invoke(messages) CODE_BLOCK_13 Answer tool.invoke(tool_call[&quot;args&quot;]) ","keywords":"","version":"Next"},{"title":"👐 Create agent tools","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools","content":"","keywords":"","version":"Next"},{"title":"Vector search tool​","type":1,"pageTitle":"👐 Create agent tools","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools#vector-search-tool","content":" CODE_BLOCK_2  Answer embedding_model.encode(text)   CODE_BLOCK_3  Answer get_embedding(user_query)   CODE_BLOCK_4  Answer [ { &quot;$vectorSearch&quot;: { &quot;index&quot;: VS_INDEX_NAME, &quot;path&quot;: &quot;embedding&quot;, &quot;queryVector&quot;: query_embedding, &quot;numCandidates&quot;: 150, &quot;limit&quot;: 5, } }, { &quot;$project&quot;: { &quot;_id&quot;: 0, &quot;body&quot;: 1, &quot;score&quot;: {&quot;$meta&quot;: &quot;vectorSearchScore&quot;}, } }, ]   CODE_BLOCK_5  Answer vs_collection.aggregate(pipeline)   ","version":"Next","tagName":"h2"},{"title":"Get page content tool​","type":1,"pageTitle":"👐 Create agent tools","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools#get-page-content-tool","content":" CODE_BLOCK_6  Answer {&quot;title&quot;: user_query}   CODE_BLOCK_7  Answer {&quot;_id&quot;: 0, &quot;body&quot;: 1}   CODE_BLOCK_8  Answer full_collection.find_one(query, projection)  ","version":"Next","tagName":"h2"},{"title":"📘 Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Creating agents using LangGraph​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#creating-agents-using-langgraph","content":" In this lab, we will use LangGraph by LangChain to orchestrate an AI agent for a technical documentation website. LangGraph allows you to model agentic systems as graphs. Graphs in LangGraph have the following core features:  ","version":"Next","tagName":"h2"},{"title":"Nodes​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#nodes","content":" Nodes in LangGraph are Python functions that encode the logic of your agents. They receive the current state of the graph as input, perform some computation and return an updated state.  ","version":"Next","tagName":"h3"},{"title":"Edges​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#edges","content":" Edges in LangGraph determine which graph node to execute next based on the current state of the graph. Edges can be conditional, fixed and even result in loops.  ","version":"Next","tagName":"h3"},{"title":"State​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#state","content":" Each graph has a state which is a shared data structure that all the nodes can access and make updates to. You can define custom attributes within the state depending on what parameters you want to track across the nodes of the graph.  To learn more about these concepts, refer to the LangGraph docs.  ","version":"Next","tagName":"h3"},{"title":"Using different LLM providers with LangChain​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#using-different-llm-providers-with-langchain","content":" LangChain supports different LLM providers for you to build AI applications with. Unless you are using open-source models, you typically need to obtain API keys to use the chat completion APIs offered by different LLM providers.  For this lab, we have created a serverless function that creates LLM objects for Amazon, Google and Microsoft models that you can use with LangChain and LangGraph without having to obtain API keys. However, if you would like to do this on your own, here are some resources:  Using Amazon Bedrock LLMs with LangChain Using Google LLMs with LangChain Using Microsoft LLMs with langChain ","version":"Next","tagName":"h2"},{"title":"👐 Define graph state","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-state","content":"👐 Define graph state Let's start by defining the state of the agent's graph. Run the cells under the Step 5: Define graph state section in the notebook to define the graph state for the AI agent.","keywords":"","version":"Next"},{"title":"👐 Create a vector search index","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-vector-search-index","content":"👐 Create a vector search index To retrieve documents using vector search, you must configure a vector search index on the collection you want to perform vector search against. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 3: Create a vector search index section in the notebook to create a vector search index. The answers for code blocks in this section are as follows: CODE_BLOCK_1 Answer create_index(vs_collection, VS_INDEX_NAME, model) ","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/intro","content":"Introduction Lab goals\tLearn the basics of building AI agentsWhat you'll learn\tWhat are AI agents When to use AI agents? Components of an AI agent Agent Architectures Building an AI agent Adding memory to agents Time to complete\t90 mins In the navigation bar and in some pages, you will notice some icons. Here is their meaning: Icon\tMeaning\tDescription📘\tLecture material\tIf you are following along in an instructor-led session, they probably have covered this already. 👐\tHands-on content\tGet ready to do some hands-on work. You should follow these steps. 📚\tDocumentation\tReference documentation for hands-on portions of the lab. 🦹\tAdvanced content\tThis content isn't covered during the lab, but if you're interested in learning more, you can check it out.","keywords":"","version":"Next"},{"title":"📘 Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"About the data​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#about-the-data","content":" In this lab, we are using a serverless function to import the data required by the agent's tools, into MongoDB. If you want to do this on your own, these datasets are available on Hugging Face:  mongodb-docs: Markdown versions of a small subset of MongoDB's technical documentation. This dataset is imported into a collection called full_docs. mongodb-docs-embedded: Chunked and embedded versions of the articles in the mongodb-docs dataset. This dataset is imported into a collection called chunked_docs.  To learn more about chunking and embedding, here are some resources from our Developer Center:  How to Choose the Right Chunking Strategy for Your LLM ApplicationHow to Choose the Best Embedding Model for Your LLM Application  ","version":"Next","tagName":"h2"},{"title":"Tool calling​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#tool-calling","content":" Tool calling, interchangeably called function calling allows an LLM to use external tools such as APIs, databases, specialized machine learning models etc.  In AI agents, an LLM can have access to multiple tools. Given a user query, the LLM decides which tool to invoke and the arguments for the tool call. These arguments are used to execute the tool call and the output is returned back to the LLM to inform its next steps.  The easiest way to define tools in LangChain is using the @tool decorator. The decorator makes tools out of functions by using the function name as the tool name by default, and the function's docstring as the tool's description. The tool call inturn consists of a tool name, arguments, and an optional identifier.  An example of a tool in LangChain is as follows:  @tool(&quot;search-tool&quot;, return_direct=True) def search(query: str) -&gt; str: &quot;&quot;&quot;Look up things online.&quot;&quot;&quot; return &quot;MongoDB&quot;   An example of a tool call is as follows:  { &quot;name&quot;: &quot;search-tool&quot;, &quot;args&quot;: { &quot;query&quot;: &quot;What is MongoDB?&quot; }, &quot;id&quot;: &quot;call_H5TttXb423JfoulF1qVfPN3m&quot; }   ","version":"Next","tagName":"h2"},{"title":"Vector Search in MongoDB​","type":1,"pageTitle":"📘 Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#vector-search-in-mongodb","content":" You can learn more about vector search in MongoDB here. ","version":"Next","tagName":"h2"},{"title":"🦹 Agent architectures","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/agent-architectures","content":"","keywords":"","version":"Next"},{"title":"Tool-calling​","type":1,"pageTitle":"🦹 Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#tool-calling","content":" This is the architecture most agents start with. It consists of a single LLM that has access to several tools to perform a range of tasks.    If you start with tool-calling agents, but upon thorough evaluation find that you need a more sophisticated architecture, only then consider multi-agent architectures. Bear in mind that fully autonomous multi-agent workflows mean higher costs, latency, and a system that is hard to debug, so use them with caution.  ","version":"Next","tagName":"h2"},{"title":"Supervisor​","type":1,"pageTitle":"🦹 Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#supervisor","content":" In this architecture, a single agent (supervisor) interfaces with a group of agents to determine the next course of action.    ","version":"Next","tagName":"h2"},{"title":"Network​","type":1,"pageTitle":"🦹 Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#network","content":" In this architecture, each agent can communicate with every other agent and decide which one to call next or end the execution.    ","version":"Next","tagName":"h2"},{"title":"Custom​","type":1,"pageTitle":"🦹 Agent architectures","url":"/ai-agents-lab/docs/key-concepts/agent-architectures#custom","content":" In this setup, you can decide which agents can interact with each other and how the control flows between them.   ","version":"Next","tagName":"h2"},{"title":"👐 Setup dev environment","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/dev-env/dev-env-setup","content":"👐 Setup dev environment 🦹 If you are doing this lab as part of a MongoDB GenAI Developer Day, you can continue in the environment you previously created. InstruqtGitHub CodespacesLocal Navigate to the Instruqt lab using this link. Click Continue to continue in the sandbox you created previously. In the Explorer menu, navigate to genai-devday-notebooks &gt; notebooks &gt; ai-agents-lab.ipynb This is the Jupyter Notebook you will be using throughout this lab. tip Notice that this documentation website is also linked in the Agents Lab Instructions tab of your Instruqt sandbox. Feel free to access the documentation from there instead for the rest of the lab. tip In this lab, we will be using Jupyter Notebooks, which is an interactive Python environment. If you are new to Jupyter Notebooks, use this guide to familiarize yourself with the environment. InstruqtGitHub CodespacesLocal Instruqt is a lab platform that provides cloud-based sandboxes which come pre-configured with all the tools you need to run this lab. Navigate to the Instruqt lab using this link. Fill out the form that appears and click Submit and access. Click Start to launch the lab environment. You should see a screen with a purple progress bar indicating that Instruqt is preparing a sandbox with all the required libraries for this lab and a MongoDB cluster. Once this is done, you should see a Start button at the bottom right of the screen. Click this to enter the lab. Connect to the MongoDB cluster​ Let's first connect to the MongoDB cluster that was created for you. This will allow you to view data we import into the cluster later in the lab, directly from the VSCode IDE. To do this, click the leaf icon in the left navigation bar of the IDE. This is MongoDB's VSCode extension. Under Connections, click the Local MongoDB Atlas connection. This should automatically establish a connection to the local MongoDB cluster running on port 27017. If the connection was successful, you should see a green leaf and a &quot;connected&quot; message appear around the Local MongoDB Atlas connection. You will also see the default databases in the cluster appear under Connections. Any additional databases we create during the lab will also appear here. Jupyter Notebook setup​ You will be filling code in a Jupyter Notebook during this lab, so let's get set up with that next! Within the sandbox, click on the files icon in the left navigation bar of the IDE. In the Explorer menu, navigate to genai-devday-notebooks &gt; notebooks &gt; ai-agents-lab.ipynb to open the Jupyter Notebook for this lab. Next, select the Python interpreter by clicking Select Kernel at the top right of the IDE. In the modal that appears, click Python environments... and select the interpreter that is marked as Recommended or Global Env. That's it! You're ready for the lab! tip Notice that this documentation website is also linked in the Agents Lab Instructions tab of your Instruqt sandbox. Feel free to access the documentation from there instead for the rest of the lab.","keywords":"","version":"Next"},{"title":"👐 Instantiate the LLM","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/instantiate-llm","content":"👐 Instantiate the LLM Now let's instantiate the LLM that will serve as the &quot;brain&quot; of the agent, and give it access to the tools we defined previously. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 6: Instantiate the LLM section in the notebook to initialize the LLM for the agent and give it access to tools. The answers for code blocks in this section are as follows: CODE_BLOCK_9 Answer llm.bind_tools(tools) CODE_BLOCK_10 Answer prompt | bind_tools ","keywords":"","version":"Next"},{"title":"📘 Embeddings","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/embeddings","content":"","keywords":"","version":"Next"},{"title":"Vector​","type":1,"pageTitle":"📘 Embeddings","url":"/ai-agents-lab/docs/key-concepts/embeddings#vector","content":" A vector is an array of numbers that represent magnitude and direction in a multi-dimensional space.  ","version":"Next","tagName":"h2"},{"title":"Embeddings​","type":1,"pageTitle":"📘 Embeddings","url":"/ai-agents-lab/docs/key-concepts/embeddings#embeddings","content":" Embeddings are vectors that represent a piece of information, such as text, images, audio, video etc. Embeddings capture semantic qualities of the data i.e. characteristics that capture the meaning or essence of it.  This way, if you plot data in this multi-dimensional vector space, semantically similar data, or data with similar meaning ends up close to each other.  ","version":"Next","tagName":"h2"},{"title":"Embedding models​","type":1,"pageTitle":"📘 Embeddings","url":"/ai-agents-lab/docs/key-concepts/embeddings#embedding-models","content":" Embedding models are specialized machine learning models that have been trained to convert a piece of information to these numerical encodings.  The embedding model you choose determines the number of elements in the embedding vectors, and consequently the number of dimensions required to represent them in vector space.  Embedding models vary depending on how the model was trained. Therefore, different models offer different advantages depending on your data and use case. While text embedding models are widely used, embedding models also exist for other types of data such as images, audio, and multimodal content. ","version":"Next","tagName":"h2"},{"title":"🎯 Summary","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/summary","content":"🎯 Summary Congratulations! Following this lab, you have successfully: learned what are AI agentslearned when to use AI agentslearned about different agent architecturesbuilt an AI agent with memory Here are some resources that you might find helpful: AI Learning HubGenAI Code Examples RepositoryGenAI Community Forums","keywords":"","version":"Next"},{"title":"👐 Import data into MongoDB","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/import-data/import-data","content":"👐 Import data into MongoDB The documentation agent has two tools- a vector search tool to retrieve information from documentation to answer questions, and another tool to get the content from specific documentation pages for summarization. Let's import the data required by these tools into two MongoDB collections. Run the cells under the Step 2: Import data into MongoDB section in the notebook to import the data required by the agent's tools, into MongoDB collections. To visually verify that the data has been imported into the local MongoDB cluster, click the leaf icon in the left navigation bar of the IDE. Ensure that you see a database called mongodb_genai_devday_agents, and two collections named mongodb_docs and mongodb_docs_embeddings under it. Click the &gt; arrow next to each collection and note the number of documents in it.","keywords":"","version":"Next"},{"title":"👐 Setup prerequisites","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/dev-env/setup-pre-reqs","content":"👐 Setup prerequisites Select the LLM provider recommended by your instructor, and run the cells under the Step 1: Setup prerequisites section in the notebook. info Additional steps if you are running the lab locally: Spin up a MongoDB Atlas cluster and obtain its connection string: Register for a free MongoDB Atlas account if you don't already have oneCreate a new database clusterObtain the connection string for your database cluster Set the MONGODB_URI variable to the connection string for your cluster as follows: MONGODB_URI = &quot;&lt;your_connection_string&gt;&quot; Manually set the value of the SERVERLESS_URL variable as follows: SERVERLESS_URL = &quot;https://vtqjvgchmwcjwsrela2oyhlegu0hwqnw.lambda-url.us-west-2.on.aws/&quot; ","keywords":"","version":"Next"},{"title":"📘 What are AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/what-are-ai-agents","content":"📘 What are AI agents? An AI agent is a system that uses an LLM to reason through a problem, create a plan to solve the problem, and execute and iterate on the plan with the help of a set of tools.","keywords":"","version":"Next"},{"title":"👐 Build and execute the graph","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/build-and-execute-graph","content":"👐 Build and execute the graph Now that we have defined the nodes and edges of the graph, let's put the graph together and execute it to ensure that the agent is working as expected. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 9: Build the graph and Step 10: Execute the graph sections in the notebook to build and execute the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_14 Answer graph.add_node(&quot;agent&quot;, agent) CODE_BLOCK_15 Answer graph.add_node(&quot;tools&quot;, tool_node) CODE_BLOCK_16 Answer graph.add_edge(START, &quot;agent&quot;) CODE_BLOCK_17 Answer graph.add_edge(&quot;tools&quot;, &quot;agent&quot;) CODE_BLOCK_18 Answer graph.add_conditional_edges( &quot;agent&quot;, route_tools, {&quot;tools&quot;: &quot;tools&quot;, END: END}, ) caution Upon executing the graph, if you see the agent get stuck in an infinite tool-calling loop, play around with the prompt in Step 7 and run the cells that follow until you initialize the llm_with_tools variable. If this doesn't work, consider this a lesson in working with non-deterministic ML models. 🙂","keywords":"","version":"Next"},{"title":"📘 When to use AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents","content":"","keywords":"","version":"Next"},{"title":"Key indicators that you need an AI agent​","type":1,"pageTitle":"📘 When to use AI agents?","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents#key-indicators-that-you-need-an-ai-agent","content":" Consider using AI agents when your task involves:  Multiple sequential steps - Tasks that require breaking down into subtasks and executing them in orderDynamic decision-making - Situations where the next action depends on the results of previous actionsTool integration - Need to interact with APIs, databases, or external systemsIterative refinement - Tasks that benefit from trying, evaluating, and adjusting approachesContext accumulation - Building understanding through multiple interactions or data sourcesPersonalization over time - Learning from user interactions to provide tailored experiences  ","version":"Next","tagName":"h2"},{"title":"When NOT to use AI agents​","type":1,"pageTitle":"📘 When to use AI agents?","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents#when-not-to-use-ai-agents","content":" AI agents add complexity and latency. Avoid them for:  Simple Q&amp;A - Questions answerable with existing LLM knowledgeStatic retrieval - Tasks solvable with basic RAGSingle-step operations - Direct API calls or database queriesReal-time requirements - Tasks needing sub-second responses  ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"📘 When to use AI agents?","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Tasks that DO NOT require an AI agent:​","type":1,"pageTitle":"📘 When to use AI agents?","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents#tasks-that-do-not-require-an-ai-agent","content":" Who was the first president of the United States?  The information required to answer this question is very likely present in the parametric knowledge of most LLMs. Hence, this question can be answer using a simple prompt to an LLM.  What is the reimbursement policy for meals for my company?  The information required to answer this question is most likely not present in the parametric knowledge of available LLMs. However, this question can easily be answered using Retrieval Augmented Generation (RAG) using a knowledge base consisting of your company's data. This still does not require an agent.  ","version":"Next","tagName":"h3"},{"title":"Tasks that DO require an AI agent:​","type":1,"pageTitle":"📘 When to use AI agents?","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents#tasks-that-do-require-an-ai-agent","content":" How has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates? Additionally, can you provide a graphical representation of the trend in obesity rates over this period?  This question involves multiple sub-tasks such as data aggregation, visualization, and reasoning. Hence, this is a good use case for an AI agent.  Creating a personalized learning assistant that can adjust its language, examples, and methods based on the student’s responses.  This is an example of a complex task which also involves user personalization. Hence, this is a good fit for an AI agent. ","version":"Next","tagName":"h3"},{"title":"📘 Vector Search","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/vector-search","content":"","keywords":"","version":"Next"},{"title":"Vector search​","type":1,"pageTitle":"📘 Vector Search","url":"/ai-agents-lab/docs/key-concepts/vector-search#vector-search","content":" Vector search, a.k.a. semantic search is an information retrieval technique that retrieves data based on intent or meaning.  Unlike traditional full-text search which finds keyword matches, vector search uses embeddings to find items closest to your search query in multi-dimensional vector space. The closer the embeddings are to your query, the more similar they are in meaning.  ","version":"Next","tagName":"h2"},{"title":"Vector search in MongoDB​","type":1,"pageTitle":"📘 Vector Search","url":"/ai-agents-lab/docs/key-concepts/vector-search#vector-search-in-mongodb","content":" In MongoDB, you can semantically search through your data using MongoDB Atlas Vector Search.  ","version":"Next","tagName":"h2"},{"title":"Sample Document with Embeddings​","type":1,"pageTitle":"📘 Vector Search","url":"/ai-agents-lab/docs/key-concepts/vector-search#sample-document-with-embeddings","content":" Before diving into how to index and search, let's look at what a document with embeddings actually looks like in MongoDB. Here's a typical document that contains both text content and its corresponding vector embedding:  { &quot;_id&quot;: &quot;doc_001&quot;, &quot;title&quot;: &quot;Introduction to Machine Learning&quot;, &quot;content&quot;: &quot;Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms that iteratively learn from data to improve their accuracy.&quot;, &quot;metadata&quot;: { &quot;author&quot;: &quot;Tech Education Team&quot;, &quot;category&quot;: &quot;AI/ML&quot;, &quot;pages&quot;: 42, &quot;published_date&quot;: &quot;2024-03-15&quot; }, &quot;embedding&quot;: [ 0.02421053, -0.022372592, 0.015163446, -0.008812144, 0.013908824, -0.029230956, 0.018722257, -0.016831137, 0.006148927, 0.011683439, // ... truncated for brevity - typically 1536 dimensions for OpenAI embeddings 0.003421892, -0.017234589 ] }   In this example:  Text Fields: The title and content fields contain the human-readable textEmbedding Field: The embedding array contains the vector representation of the text content These numbers were generated by passing the text through an embedding modelEach number represents a dimension in the vector spaceThe length of this array (number of dimensions) depends on the embedding model used Metadata Fields: Additional fields like author, category, and pages can be used for filtering during vector search  The embedding captures the semantic meaning of the text. Documents with similar content will have embeddings that are close together in the vector space, which is what makes semantic search possible.  ","version":"Next","tagName":"h3"},{"title":"Creating a Vector Search Index​","type":1,"pageTitle":"📘 Vector Search","url":"/ai-agents-lab/docs/key-concepts/vector-search#creating-a-vector-search-index","content":" To perform vector search on your data in MongoDB, you need to create a vector search index. An example of a vector search index definition looks as follows:  { &quot;fields&quot;:[ { &quot;type&quot;: &quot;vector&quot;, &quot;path&quot;: &quot;embedding&quot;, &quot;numDimensions&quot;: 1536, &quot;similarity&quot;: &quot;cosine&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;pages&quot; }, ... ] }   In the index definition, you specify the path to the embedding field (path), the number of dimensions in the embedding vectors (numDimensions), and a similarity metric that specifies how to determine nearest neighbors in vector space (similarity). You can also index filter fields that allow you to pre-filter on certain metadata to narrow the scope of the vector search.  Vector search in MongoDB takes the form of an aggregation pipeline stage. It always needs to be the first stage in the pipeline and can be followed by other stages to further process the semantic search results. An example pipeline including the $vectorSearch stage is as follows:  [ { &quot;$vectorSearch&quot;: { &quot;index&quot;: &quot;vector_index&quot;, &quot;path&quot;: &quot;embedding&quot;, &quot;queryVector&quot;: [0.02421053, -0.022372592,...], &quot;numCandidates&quot;: 150, &quot;filter&quot;: {&quot;pages&quot;: 100}, &quot;limit&quot;: 10 } }, { &quot;$project&quot;: { &quot;_id&quot;: 0, &quot;title&quot;: 1, &quot;score&quot;: {&quot;$meta&quot;: &quot;vectorSearchScore&quot;} } } ]   In this example, you can see a vector search query with a pre-filter. The limit field in the query definition specifies how many documents to return from the vector search.  The $project stage that follows only returns documents with the title field and the similarity score from the vector search. ","version":"Next","tagName":"h3"},{"title":"📘 Components of AI agents","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/components-of-agents","content":"","keywords":"","version":"Next"},{"title":"Perception​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#perception","content":" Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs.  ","version":"Next","tagName":"h2"},{"title":"Planning and reasoning​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#planning-and-reasoning","content":" AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed.  Before diving into specific reasoning patterns, it's important to understand prompting strategies:  ","version":"Next","tagName":"h2"},{"title":"Prompting Strategies​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#prompting-strategies","content":" Zero-Shot Prompting​  The model is given a task without any examples, relying entirely on its pre-trained knowledge.  Example:  Classify the sentiment of this text as positive, negative, or neutral: &quot;The product exceeded my expectations.&quot;   Pros:  No need to craft examplesFast and straightforwardWorks well for simple, common tasksMinimal prompt engineering required  Cons:  May struggle with complex or domain-specific tasksLess consistent output formatNo guidance on desired style or approachPerformance varies significantly by task  One-Shot Prompting​  The model is provided with exactly one example before performing the task.  Example:  Classify sentiment as positive, negative, or neutral. Example: Text: &quot;The service was terrible and slow.&quot; Sentiment: negative Now classify: Text: &quot;The product exceeded my expectations.&quot; Sentiment:   Pros:  Provides clear format guidanceBetter consistency than zero-shotHelps establish the desired output styleStill relatively concise  Cons:  Single example may not cover edge casesCan bias the model toward the example's specific patternMay not generalize well to diverse inputsLimited demonstration of task complexity  Few-Shot / Many-Shot / Multi-Shot Prompting​  The model is given multiple examples (typically 3-10 for few-shot, more for many-shot) to learn the pattern.  Example:  Classify sentiment as positive, negative, or neutral. Example 1: Text: &quot;The service was terrible and slow.&quot; Sentiment: negative Example 2: Text: &quot;Amazing product, highly recommend!&quot; Sentiment: positive Example 3: Text: &quot;The item works as described.&quot; Sentiment: neutral Now classify: Text: &quot;The product exceeded my expectations.&quot; Sentiment:   Pros:  Best performance on complex tasksCan demonstrate edge cases and nuancesEstablishes clear patterns and consistencyReduces ambiguity in task interpretation  Cons:  Requires careful example selectionUses more tokens (higher cost)Risk of overfitting to examplesCan hit context length limits with many examplesTime-consuming to create good examples  ","version":"Next","tagName":"h3"},{"title":"Common Design Patterns​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#common-design-patterns","content":" With these prompting strategies in mind, here are common design patterns for planning and reasoning in AI agents:  ","version":"Next","tagName":"h3"},{"title":"Chain of Thought (CoT) Prompting​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#chain-of-thought-cot-prompting","content":" In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem. CoT prompting helps the model break down complex problems into manageable steps and show its reasoning process transparently.  Pros:  Improves accuracy on complex reasoning tasksMakes the model's thinking process transparent and auditableHelps catch logical errors in the reasoning chainWorks well for mathematical, logical, and multi-step problems  Cons:  Increases token usage and response timeMay overthink simple problemsCan sometimes generate incorrect reasoning that leads to wrong answersNot necessary for straightforward tasks  Example:  Prompt: Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Question: A store has 120 apples. They sell 40% on Monday and 25% of the remainder on Tuesday. How many apples are left? Response: Step 1: Calculate apples sold on Monday - 40% of 120 = 0.4 × 120 = 48 apples sold Step 2: Calculate remainder after Monday - 120 - 48 = 72 apples remaining Step 3: Calculate apples sold on Tuesday - 25% of 72 = 0.25 × 72 = 18 apples sold Step 4: Calculate final remainder - 72 - 18 = 54 apples left Answer: 54 apples remain after Tuesday's sales.   ","version":"Next","tagName":"h3"},{"title":"ReAct (Reason + Act)​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#react-reason--act","content":" While Chain of Thought focuses purely on internal reasoning, ReAct combines reasoning with action-taking. The key difference is that ReAct:  Interleaves reasoning with actions: The agent thinks, acts, observes the results, then thinks again based on new informationGrounds reasoning in real-world feedback: Unlike pure CoT which reasons in isolation, ReAct agents can validate their reasoning by interacting with tools and observing outcomesEnables dynamic adaptation: The agent can adjust its approach based on actual results rather than theoretical reasoning alone  In ReAct, the LLM alternates between:  Thought: Internal reasoning about what to do nextAction: Executing a specific tool or API callObservation: Processing the results of that action  This cycle continues until the task is complete, making ReAct particularly effective for tasks that require gathering information from external sources or verifying hypotheses through interaction.  Pros:  Can access real-time, up-to-date information through toolsSelf-corrects based on actual feedbackMore reliable for tasks requiring external dataReduces hallucination by grounding responses in tool outputs  Cons:  More complex to implement than pure CoTRequires careful tool design and error handlingCan get stuck in loops if not properly constrainedHigher latency due to multiple tool callsMore expensive due to additional API calls and tokens  Example:  Prompt: Answer the following questions as best you can. You have access to the following tools: - Weather: Gets current weather for a location - Calculator: Performs mathematical calculations Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [Weather, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Question: What is the current temperature in Tokyo, and how much warmer is it than the freezing point of water in Celsius? Response: Thought: I need to first get the current temperature in Tokyo, then calculate the difference from 0°C (freezing point). Action: Weather Action Input: Tokyo, Japan Observation: Current temperature in Tokyo is 22°C, partly cloudy Thought: Now I need to calculate how much warmer 22°C is compared to the freezing point of 0°C. Action: Calculator Action Input: 22 - 0 Observation: 22 Thought: I now know the final answer Final Answer: The current temperature in Tokyo is 22°C, which is 22°C warmer than the freezing point of water (0°C).   Notice how the agent cycles through Thought → Action → Observation multiple times, using each observation to inform the next thought and action. This demonstrates the key difference from Chain of Thought, which would reason through the entire problem internally without external tool interactions.  ","version":"Next","tagName":"h3"},{"title":"Reflection​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#reflection","content":" Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality.  Pros:  Improves output quality through iterative refinementCan catch and correct mistakes before final outputParticularly effective for creative and analytical tasksReduces errors through self-critique  Cons:  Significantly increases computational cost and latencyMay over-revise and lose good initial insightsNot always necessary for simple tasksCan introduce new errors during revision  Example:  Prompt: You will iteratively improve your response through reflection. Follow this format: 1. Initial Attempt: Your first response 2. Reflection: Critique your attempt, identifying strengths and weaknesses 3. Revised Attempt: An improved version based on your reflection 4. Final Reflection: Evaluate if further improvements are needed 5. Final Answer: Your best version Question: Write a haiku about artificial intelligence Response: Initial Attempt: Circuits pulse with light Digital minds awakening Future now unfolds Reflection: Let me review this haiku. It follows the 5-7-5 syllable pattern correctly. However, &quot;awakening&quot; might be cliché when describing AI. Also, the imagery could be more concrete and less abstract. Revised Attempt: Silicon dreams deep Patterns learned from human words Teaching us ourselves Final Reflection: This version is better - it's less cliché and includes the meta aspect of AI learning from humans. The last line has a nice philosophical touch about AI reflecting humanity back to us. The syllable count is correct (5-7-5). This captures both the technical and philosophical aspects of AI effectively. Final Answer: Silicon dreams deep Patterns learned from human words Teaching us ourselves   Notice how the agent generates, critiques, and refines its output through multiple iterations, improving quality with each reflection cycle.  ","version":"Next","tagName":"h3"},{"title":"Combining Patterns​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#combining-patterns","content":" These patterns can be combined for more sophisticated reasoning. Here's an example using all three:  Example: CoT + ReAct + Reflection Combined  Prompt: You will solve problems using a combination of reasoning patterns: 1. Use Chain of Thought for complex calculations 2. Use ReAct format when you need external tools 3. Use Reflection to verify and improve your final answer Available tools: [Weather, Calculator, UnitConverter] Question: If it's currently 28°C in Singapore and -5°C in Toronto, what's the temperature difference in Fahrenheit? Also, at which temperature would water boil at sea level in both cities? Response: Initial Approach: Thought: I need to get the temperatures, convert them, and calculate the boiling point. Let me start by understanding what I need to do. Step 1 (CoT): Break down the problem - Get current temperatures (already provided: Singapore 28°C, Toronto -5°C) - Calculate temperature difference in Celsius - Convert difference to Fahrenheit - Determine boiling point of water at sea level Step 2 (ReAct): Execute with tools Thought: First, let me calculate the temperature difference in Celsius Action: Calculator Action Input: 28 - (-5) Observation: 33 Thought: Now I need to convert this 33°C difference to Fahrenheit. The formula is (C × 9/5) + 32, but for a difference, I only need C × 9/5 Action: Calculator Action Input: 33 * 9 / 5 Observation: 59.4 Thought: Now I need to get the boiling point of water at sea level Action: UnitConverter Action Input: 100°C to Fahrenheit Observation: 212°F Step 3 (Reflection): Review my work Initial Answer: The temperature difference is 59.4°F, and water boils at 212°F (100°C) at sea level in both cities. Reflection: Wait, I should verify my calculation. For temperature differences, the conversion is just multiplying by 9/5, which I did correctly (33 × 1.8 = 59.4°F). The boiling point is correct at sea level. However, I should clarify that the boiling point is the same in both cities since they're both at approximately sea level. Final Answer: - Temperature difference: 59.4°F (33°C difference) - Water boils at 100°C (212°F) at sea level in both Singapore and Toronto   This example demonstrates how:  Chain of Thought helps break down the complex problem into stepsReAct enables interaction with tools for calculations and conversionsReflection catches potential ambiguities and improves the final answer  These patterns work synergistically: CoT provides the planning, ReAct executes the plan with real-world interactions, and Reflection ensures quality and accuracy.  ","version":"Next","tagName":"h3"},{"title":"Tools​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#tools","content":" Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models.  ","version":"Next","tagName":"h2"},{"title":"Memory​","type":1,"pageTitle":"📘 Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#memory","content":" The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions.  There are two main types of memory for AI agents:  Short-term memory: Stores and retrieves information from a specific conversation. Long-term memory: Stores, retrieves and updates information based on multiple conversations had over a period of time. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}